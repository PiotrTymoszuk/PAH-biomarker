@article{Wickham2019,
abstract = {At a high level, the tidyverse is a language for solving data science challenges with R code. Its primary goal is to facilitate a conversation between a human and a computer about data. Less abstractly, the tidyverse is a collection of R packages that share a high-level design philosophy and low-level grammar and data structures, so that learning one package makes it easier to learn the next. The tidyverse encompasses the repeated tasks at the heart of every data science project: data import, tidying, manipulation, visualisation, and programming. We expect that almost every project will use multiple domain-specific packages outside of the tidyverse: our goal is to provide tooling for the most common challenges; not to solve every possible problem. Notably, the tidyverse doesn't include tools for statistical modelling or communication. These toolkits are critical for data science, but are so large that they merit separate treatment. The tidyverse package allows users to install all tidyverse packages with a single command. There are a number of projects that are similar in scope to the tidyverse. The closest is perhaps Bioconductor (Gentleman et al., 2004; Huber et al., 2015), which provides an ecosystem of packages that support the analysis of high-throughput genomic data.},
author = {Wickham, Hadley and Averick, Mara and Bryan, Jennifer and Chang, Winston and McGowan, Lucy and Fran{\c{c}}ois, Romain and Grolemund, Garrett and Hayes, Alex and Henry, Lionel and Hester, Jim and Kuhn, Max and Pedersen, Thomas and Miller, Evan and Bache, Stephan and M{\"{u}}ller, Kirill and Ooms, Jeroen and Robinson, David and Seidel, Dana and Spinu, Vitalie and Takahashi, Kohske and Vaughan, Davis and Wilke, Claus and Woo, Kara and Yutani, Hiroaki},
doi = {10.21105/joss.01686},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wickham et al. - 2019 - Welcome to the Tidyverse.pdf:pdf},
issn = {2475-9066},
journal = {Journal of Open Source Software},
month = {nov},
number = {43},
pages = {1686},
publisher = {The Open Journal},
title = {{Welcome to the Tidyverse}},
volume = {4},
year = {2019}
}
@article{Lopez-Raton2014,
abstract = {Continuous diagnostic tests are often used for discriminating between healthy and diseased populations. For the clinical application of such tests, it is useful to select a cutpoint or discrimination value c that deffnes positive and negative test results. In general, individuals with a diagnostic test value of c or higher are classiffed as diseased. Several search strategies have been proposed for choosing optimal cutpoints in diagnostic tests, depending on the underlying reason for this choice. This paper introduces an R package, known as OptimalCutpoints, for selecting optimal cutpoints in diagnostic tests. It incorporates criteria that take the costs of the different diagnostic decisions into account, as well as the prevalence of the target disease and several methods based on measures of diagnostic test accuracy. Moreover, it enables optimal levels to be calculated according to levels of given (categorical) covariates. While the numerical output includes the optimal cutpoint values and associated accuracy measures with their conffdence intervals, the graphical output includes the receiver operating characteristic (ROC) and predictive ROC curves. An illustration of the use of OptimalCutpoints is provided, using a real biomedical dataset.},
author = {L{\'{o}}pez-Rat{\'{o}}n, M{\'{o}}nica and Rodr{\'{i}}guez-{\'{A}}lvarez, Mar{\'{i}}a Xos{\'{e}} and Cadarso-Su{\'{a}}rez, Carmen and Gude-Sampedro, Francisco},
doi = {10.18637/jss.v061.i08},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/L{\'{o}}pez-Rat{\'{o}}n et al. - 2014 - Optimalcutpoints An R package for selecting optimal cutpoints in diagnostic tests.pdf:pdf},
issn = {15487660},
journal = {Journal of Statistical Software},
keywords = {Accuracy measures,Diagnostic tests,Optimal cutpoint,R,ROC curve},
month = {nov},
number = {8},
pages = {1--36},
publisher = {American Statistical Association},
title = {{Optimalcutpoints: An R package for selecting optimal cutpoints in diagnostic tests}},
url = {https://www.jstatsoft.org/index.php/jss/article/view/v061i08/v61i08.pdf https://www.jstatsoft.org/index.php/jss/article/view/v061i08},
volume = {61},
year = {2014}
}
@book{Xie2016,
abstract = {"A Chapman & Hall book." Bookdown: Authoring Books and Technical Documents with R Markdown presents a much easier way to write books and technical publications than traditional tools such as LaTeX and Word. The bookdown package inherits the simplicity of syntax and flexibility for data analysis from R Markdown, and extends R Markdown for technical writing, so that you can make better use of document elements such as figures, tables, equations, theorems, citations, and references. Similar to LaTeX, you can number and cross-reference these elements with bookdown. Your document can even include live examples so readers can interact with them while reading the book. The book can be rendered to multiple output formats, including LaTeX/PDF, HTML, EPUB, and Word, thus making it easy to put your documents online. The style and theme of these output formats can be customized. We used books and R primarily for examples in this book, but bookdown is not only for books or R. Most features introduced in this book also apply to other types of publications: journal papers, reports, dissertations, course handouts, study notes, and even novels. You do not have to use R, either. Other choices of computing languages include Python, C, C plus plus, SQL, Bash, Stan, JavaScript, and so on, although R is best supported. You can also leave out computing, for example, to write a fiction. This book itself is an example of publishing with bookdown and R Markdown, and its source is fully available on GitHub. 1. Introduction -- 2. Components -- 3. Output formats -- 4. Customization -- 5. Editing -- 6. Publishing.},
author = {Xie, Yihui},
isbn = {9781138700109},
pages = {113},
title = {{Bookdown : authoring books and technical documents with R Markdown}},
year = {2016}
}
@book{Wilke2019,
abstract = {First edition. Intro; Copyright; Table of Contents; Preface; Thoughts on Graphing Software and Figure-Preparation Pipelines; Conventions Used in This Book; Using Code Examples; O'Reilly Online Learning; How to Contact Us; Acknowledgments; Chapter 1. Introduction; Ugly, Bad, and Wrong Figures; Part I. From Data to Visualization; Chapter 2. Visualizing Data: Mapping Data onto Aesthetics; Aesthetics and Types of Data; Scales Map Data Values onto Aesthetics; Chapter 3. Coordinate Systems and Axes; Cartesian Coordinates; Nonlinear Axes; Coordinate Systems with Curved Axes; Chapter 4. Color Scales Color as a Tool to DistinguishColor to Represent Data Values; Color as a Tool to Highlight; Chapter 5. Directory of Visualizations; Amounts; Distributions; Proportions; x-y relationships; Geospatial Data; Uncertainty; Chapter 6. Visualizing Amounts; Bar Plots; Grouped and Stacked Bars; Dot Plots and Heatmaps; Chapter 7. Visualizing Distributions: Histograms and Density Plots; Visualizing a Single Distribution; Visualizing Multiple Distributions at the Same Time; Chapter 8. Visualizing Distributions: Empirical Cumulative Distribution Functions and Q-Q Plots Empirical Cumulative Distribution FunctionsHighly Skewed Distributions; Quantile-Quantile Plots; Chapter 9. Visualizing Many Distributions at Once; Visualizing Distributions Along the Vertical Axis; Visualizing Distributions Along the Horizontal Axis; Chapter 10. Visualizing Proportions; A Case for Pie Charts; A Case for Side-by-Side Bars; A Case for Stacked Bars and Stacked Densities; Visualizing Proportions Separately as Parts of the Total; Chapter 11. Visualizing Nested Proportions; Nested Proportions Gone Wrong; Mosaic Plots and Treemaps; Nested Pies; Parallel Sets Chapter 12. Visualizing Associations Among Two or More Quantitative VariablesScatterplots; Correlograms; Dimension Reduction; Paired Data; Chapter 13. Visualizing Time Series and Other Functions of an Independent Variable; Individual Time Series; Multiple Time Series and Dose-Response Curves; Time Series of Two or More Response Variables; Chapter 14. Visualizing Trends; Smoothing; Showing Trends with a Defined Functional Form; Detrending and Time-Series Decomposition; Chapter 15. Visualizing Geospatial Data; Projections; Layers; Choropleth Mapping; Cartograms Chapter 16. Visualizing UncertaintyFraming Probabilities as Frequencies; Visualizing the Uncertainty of Point Estimates; Visualizing the Uncertainty of Curve Fits; Hypothetical Outcome Plots; Part II. Principles of Figure Design; Chapter 17. The Principle of Proportional Ink; Visualizations Along Linear Axes; Visualizations Along Logarithmic Axes; Direct Area Visualizations; Chapter 18. Handling Overlapping Points; Partial Transparency and Jittering; 2D Histograms; Contour Lines; Chapter 19. Common Pitfalls of Color Use; Encoding Too Much or Irrelevant Information},
address = {Sebastopol},
author = {Wilke, Claus O},
booktitle = {O'Reilly Media},
edition = {1},
isbn = {1492031089},
pages = {389},
publisher = {O'Reilly Media},
title = {{Fundamentals of Data Visualization: A Primer on Making Informative and Compelling Figures}},
year = {2019}
}
@article{Murtagh2012,
abstract = {We survey agglomerative hierarchical clustering algorithms and discuss efficient implementations that are available in R and other software environments. We look at hierarchical self-organizing maps, and mixture models. We review grid-based clustering, focusing on hierarchical density-based approaches. Finally, we describe a recently developed very efficient (linear time) hierarchical clustering algorithm, which can also be viewed as a hierarchical grid-based algorithm. {\textcopyright} 2011 Wiley Periodicals, Inc.},
author = {Murtagh, Fionn and Contreras, Pedro},
doi = {10.1002/widm.53},
issn = {19424787},
journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
month = {jan},
number = {1},
pages = {86--97},
publisher = {John Wiley & Sons, Ltd},
title = {{Algorithms for hierarchical clustering: An overview}},
url = {https://onlinelibrary.wiley.com/doi/full/10.1002/widm.53 https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.53 https://onlinelibrary.wiley.com/doi/10.1002/widm.53},
volume = {2},
year = {2012}
}
@article{Brunson2020,
author = {Brunson, Jason},
doi = {10.21105/joss.02017},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Brunson - 2020 - ggalluvial Layered Grammar for Alluvial Plots.pdf:pdf},
issn = {2475-9066},
journal = {Journal of Open Source Software},
month = {may},
number = {49},
pages = {2017},
publisher = {The Open Journal},
title = {{ggalluvial: Layered Grammar for Alluvial Plots}},
url = {https://joss.theoj.org/papers/10.21105/joss.02017},
volume = {5},
year = {2020}
}
@article{Sonnweber2020,
abstract = {BACKGROUND After the 2002/2003 SARS outbreak, 30% of survivors exhibited persisting structural pulmonary abnormalities. The long-term pulmonary sequelae of coronavirus disease 2019 (COVID-19) are yet unknown, and comprehensive clinical follow-up data are lacking. METHODS In this prospective, multicentre, observational study, we systematically evaluated the cardiopulmonary damage in subjects recovering from COVID-19 at 60 and 100 days after confirmed diagnosis. We conducted a detailed questionnaire, clinical examination, laboratory testing, lung function analysis, echocardiography, and thoracic low-dose computed tomography (CT). RESULTS Data from 145 COVID-19 patients were evaluated, and 41% of all subjects exhibited persistent symptoms 100 days after COVID-19 onset, with dyspnea being most frequent (36%). Accordingly, patients still displayed an impaired lung function, with a reduced diffusing capacity in 21% of the cohort being the most prominent finding. Cardiac impairment, including a reduced left ventricular function or signs of pulmonary hypertension, was only present in a minority of subjects. CT scans unveiled persisting lung pathologies in 63% of patients, mainly consisting of bilateral ground-glass opacities and/or reticulation in the lower lung lobes, without radiological signs of pulmonary fibrosis. Sequential follow-up evaluations at 60 and 100 days after COVID-19 onset demonstrated a vast improvement of both, symptoms and CT abnormalities over time. CONCLUSION A relevant percentage of post-COVID-19 patients presented with persisting symptoms and lung function impairment along with pulmonary abnormalities more than 100 days after the diagnosis of COVID-19. However, our results indicate a significant improvement in symptoms and cardiopulmonary status over time.},
author = {Sonnweber, Thomas and Sahanic, Sabina and Pizzini, Alex and Luger, Anna and Schwabl, Christoph and Sonnweber, Bettina and Kurz, Katharina and Koppelst{\"{a}}tter, Sabine and Haschka, David and Petzer, Verena and Boehm, Anna and Aichner, Magdalena and Tymoszuk, Piotr and Lener, Daniela and Theurl, Markus and Lorsbach-K{\"{o}}hler, Almut and Tancevski, Amra and Schapfl, Anna and Schaber, Marc and Hilbe, Richard and Nairz, Manfred and Puchner, Bernhard and H{\"{u}}ttenberger, Doris and Tschurtschenthaler, Christoph and A{\ss}hoff, Malte and Peer, Andreas and Hartig, Frank and Bellmann, Romuald and Joannidis, Michael and Gollmann-Tepek{\"{o}}yl{\"{u}}, Can and Holfeld, Johannes and Feuchtner, Gudrun and Egger, Alexander and Hoermann, Gregor and Schroll, Andrea and Fritsche, Gernot and Wildner, Sophie and Bellmann-Weiler, Rosa and Kirchmair, Rudolf and Helbok, Raimund and Prosch, Helmut and Rieder, Dietmar and Trajanoski, Zlatko and Kronenberg, Florian and W{\"{o}}ll, Ewald and Weiss, G{\"{u}}nter and Widmann, Gerlig and L{\"{o}}ffler-Ragg, Judith and Tancevski, Ivan},
doi = {10.1183/13993003.03481-2020},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sonnweber et al. - 2020 - Cardiopulmonary recovery after COVID-19 - an observational prospective multi-center trial.pdf:pdf},
issn = {1399-3003},
journal = {The European respiratory journal},
keywords = {Ivan Tancevski,MEDLINE,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,PMC7736754,PubMed Abstract,Sabina Sahanic,Thomas Sonnweber,doi:10.1183/13993003.03481-2020,pmid:33303539},
month = {dec},
pmid = {33303539},
publisher = {Eur Respir J},
title = {{Cardiopulmonary recovery after COVID-19 - an observational prospective multi-center trial.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/33303539},
year = {2020}
}
@article{Croux2007,
abstract = {The results of a standard principal component analysis (PCA) can be affected by the presence of outliers. Hence robust alternatives to PCA are needed. One of the most appealing robust methods for principal component analysis uses the Projection-Pursuit principle. Here, one projects the data on a lower-dimensional space such that a robust measure of variance of the projected data will be maximized. The Projection-Pursuit-based method for principal component analysis has recently been introduced in the field of chemometrics, where the number of variables is typically large. In this paper, it is shown that the currently available algorithm for robust Projection-Pursuit PCA performs poor in the presence of many variables. A new algorithm is proposed that is more suitable for the analysis of chemical data. Its performance is studied by means of simulation experiments and illustrated on some real data sets. {\textcopyright} 2007 Elsevier B.V. All rights reserved.},
author = {Croux, C. and Filzmoser, P. and Oliveira, M. R.},
doi = {10.1016/j.chemolab.2007.01.004},
issn = {01697439},
journal = {Chemometrics and Intelligent Laboratory Systems},
keywords = {Multivariate statistics,Numerical precision,Optimization,Outliers,Robustness,Scale estimators},
month = {jun},
number = {2},
pages = {218--225},
publisher = {Elsevier},
title = {{Algorithms for Projection-Pursuit robust principal component analysis}},
volume = {87},
year = {2007}
}
@inproceedings{Ester1996,
abstract = {Clustering algorithms are attractive for the task of class identification in spatial databases. However, the application to large spatial databases rises the following requirements for clustering algorithms: minimal requirements of domain knowledge to determine the input parameters, discovery of clusters with arbitrary shape and good efficiency on large databases. The well-known clustering algorithms offer no solution to the combination of these requirements. In this paper, we present the new clustering algorithm DBSCAN relying on a density-based notion of clusters which is designed to discover clusters of arbitrary shape. DBSCAN requires only one input parameter and supports the user in determining an appropriate value for it. We performed an experimental evaluation of the effectiveness and efficiency of DBSCAN using synthetic data and real data of the SEQUOIA 2000 benchmark. The results of our experiments demonstrate that (1) DBSCAN is significantly more effective in discovering clusters of arbitrary shape than the well-known algorithm CLAR-ANS, and that (2) DBSCAN outperforms CLARANS by a factor of more than 100 in terms of efficiency.},
author = {Ester, Martin and Kriegel, Hans-Peter and Sander, J{\"{o}}rg and Xu, Xiaowei},
booktitle = {Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining},
doi = {10.5555/3001460.3001507},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ester et al. - 1996 - A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise.pdf:pdf},
keywords = {Arbitrary Shape of Clus-ters,Clustering Algorithms,Efficiency on Large Spatial Databases,Handling Nlj4-275oise},
pages = {226--231},
title = {{A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise}},
url = {www.aaai.org},
year = {1996}
}
@article{Belyadi2016,
author = {Belyadi, Hoss and Haghighat, Alireza and Nguyen, Ha and Guerin, Al -Joris},
doi = {10.1088/1755-1315/31/1/012012},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Belyadi et al. - 2016 - IOP Conference Series Earth and Environmental Science Determination of Optimal Epsilon (Eps) Value on DBSCAN Alg.pdf:pdf},
journal = {IOP Conf. Ser.: Earth Environ. Sci},
title = {{IOP Conference Series: Earth and Environmental Science Determination of Optimal Epsilon (Eps) Value on DBSCAN Algorithm to Clustering Data on Peatland Hotspots in Sumatra Related content EPS conference comes to London-EPS rewards quasiparticle research-EP}},
volume = {31},
year = {2016}
}
@book{Wickham2016,
address = {New York},
author = {Wickham, Hadley.},
edition = {1},
isbn = {978-3-319-24277-4},
publisher = {Springer-Verlag},
title = {{ggplot2: Elegant Graphics for Data Analysis}},
url = {https://ggplot2.tidyverse.org},
year = {2016}
}
@article{Brandtner2020,
abstract = {Background: Sepsis, a dysregulated host response following infection, is associated with massive immune activation and high mortality rates. There is still a need to define further risk factors and laboratory parameters predicting the clinical course. Iron metabolism is regulated by both, the body's iron status and the immune response. Iron itself is required for erythropoiesis but also for many cellular and metabolic functions. Moreover, iron availability is a critical determinant in infections because it is an essential nutrient for most microbes but also impacts on immune function and intravascular oxidative stress. Herein, we used a prospective study design to investigate the putative impact of serum iron parameters on the outcome of sepsis. Methods: Serum markers of iron metabolism were measured in a prospective cohort of 61 patients (37 males, 24 females) with sepsis defined by Sepsis-3 criteria in a medical intensive care unit (ICU) and compared between survivors and non-survivors. Regulation of iron parameters in patients stratified by focus of infection and co-medication as well as association of the markers with sepsis severity scores and survival were investigated with linear and logistic regression corrected for sex and age effects. Results: Positive correlations of increased serum iron and ferritin concentrations upon ICU admission with the severity of organ failure (SOFA score) and with mortality were observed. Moreover, high TF-Sat, elevated ferritin and serum iron levels and low transferrin concentrations were associated with reduced survival. A logistic regression model consisting of SOFA and transferrin saturation (SOFA-TF-Sat) had the best predictive power for survival in septic ICU patients. Of note, administration of blood transfusions prior to ICU admission resulted in increased TF-Sat and reduced survival of septic patients. Conclusions: Our study could show an important impact of serum iron parameters on the outcome of sepsis. Furthermore, we identified transferrin saturation as a stand-alone predictor of sepsis survival and as a parameter of iron metabolism which may in a combined model improve the prediction power of the SOFA score. Trial registration: The study was carried out in accordance with the recommendations of the Declaration of Helsinki on biomedical research. The study was approved by the institutional ethics review board of the Medical University Innsbruck (study AN2013-0006).},
author = {Brandtner, Anna and Tymoszuk, Piotr and Nairz, Manfred and Lehner, Georg F. and Fritsche, Gernot and Vales, Anja and Falkner, Andreas and Schennach, Harald and Theurl, Igor and Joannidis, Michael and Weiss, G{\"{u}}nter and Pfeifhofer-Obermair, Christa},
doi = {10.1186/s40560-020-00495-8},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Brandtner et al. - 2020 - Linkage of alterations in systemic iron homeostasis to patients' outcome in sepsis A prospective study.pdf:pdf},
issn = {20520492},
journal = {Journal of Intensive Care},
keywords = {Ferritin,Infection,SOFA score,Transferrin,Transferrin saturation},
month = {oct},
number = {1},
publisher = {BioMed Central Ltd},
title = {{Linkage of alterations in systemic iron homeostasis to patients' outcome in sepsis: A prospective study}},
url = {https://pubmed.ncbi.nlm.nih.gov/33014378/},
volume = {8},
year = {2020}
}
@article{Hartigan1979,
abstract = {The K-means clustering algorithm is described indetail by Hartigan(1975). An efficient version of the algorithm is presented here.\nThe aim of the K-means algorithm is to divide M points in N dimensions into K clusters so that the within-cluster sum of squares is minimized. It is not practical to require that the solution has minimal sum of squares against all partitions except when M,N are small and K = 2. We seek instead "local" optima, solution such that no movement of a point from one cluster to another will reduce the within cluster sum of squares.},
author = {Hartigan, J. A. and Wong, M. A.},
doi = {10.2307/2346830},
issn = {00359254},
journal = {Applied Statistics},
number = {1},
pages = {100},
publisher = {JSTOR},
title = {{Algorithm AS 136: A K-Means Clustering Algorithm}},
volume = {28},
year = {1979}
}
@article{Benjamini1995,
abstract = {The common approach to the multiplicity problem calls for controlling the familywise error rate (FWER). This approach, though, has faults, and we point out a few. A different approach to problems of multiple significance testing is presented. It calls for controlling the expected proportion of falsely rejected hypotheses- the false discovery rate. This error rate is equivalent to the FWER when all hypotheses are true but is smaller otherwise. Therefore, in problems where the control of the false discovery rate rather than that of the FWER is desired, there is potential for a gain in power. A simple sequential Bonferronitype procedure is proved to control the false discovery rate for independent test statistics, and a simulation study shows that the gain in power is substantial. The use of the new procedure and the appropriateness of the criterion are illustrated with examples.},
author = {Benjamini, Yoav and Hochberg, Yosef},
doi = {10.1111/j.2517-6161.1995.tb02031.x},
issn = {0035-9246},
journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
keywords = {bonferroni‐type procedures,familywise error rate,multiple‐comparison procedures,p‐values},
month = {jan},
number = {1},
pages = {289--300},
publisher = {Wiley},
title = {{Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing}},
volume = {57},
year = {1995}
}
@article{Kassambara2016,
author = {Kassambara, Alboukadel and Kosinski, Marcin and Biecek, Przemyslaw},
title = {{survminer: Drawing Survival Curves using 'ggplot2'}},
url = {https://cran.r-project.org/package=survminer},
year = {2016}
}
@inproceedings{Boriah2008,
abstract = {Measuring similarity or distance between two entities is a key step for several data mining and knowledge discovery tasks. The notion of similarity for continuous data is relatively well-understood, but for categorical data, the similarity computation is not straightforward. Several data-driven similarity measures have been proposed in the literature to compute the similarity between two categorical data instances but their relative performance has not been evaluated. In this paper we study the performance of a variety of similarity measures in the context of a specific data mining task: outlier detection. Results on a variety of data sets show that while no one measure dominates others for all types of problems, some measures are able to have consistently high performance. Copyright {\textcopyright} by SIAM.},
author = {Boriah, Shyam and Chandola, Varun and Kumar, Vipin},
booktitle = {Society for Industrial and Applied Mathematics - 8th SIAM International Conference on Data Mining 2008, Proceedings in Applied Mathematics 130},
doi = {10.1137/1.9781611972788.22},
isbn = {9781605603179},
month = {oct},
pages = {243--254},
title = {{Similarity measures for categorical data: A comparative evaluation}},
url = {https://experts.umn.edu/en/publications/similarity-measures-for-categorical-data-a-comparative-evaluation},
volume = {1},
year = {2008}
}
@article{Sachs2017,
abstract = {Plots of the receiver operating characteristic (ROC) curve are ubiquitous in medical research. Designed to simultaneously display the operating characteristics at every possible value of a continuous diagnostic test, ROC curves are used in oncology to evaluate screening, diagnostic, prognostic and predictive biomarkers. I reviewed a sample of ROC curve plots from the major oncology journals in order to assess current trends in usage and design elements. My review suggests that ROC curve plots are often ineffective as statistical charts and that poor design obscures the relevant information the chart is intended to display. I describe my new R package that was created to address the shortcomings of existing tools. The package has functions to create informative ROC curve plots, with sensible defaults and a simple interface, for use in print or as an interactive web-based plot. A web application was developed to reach a broader audience of scientists who do not use R.},
author = {Sachs, Michael C.},
doi = {10.18637/jss.v079.c02},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sachs - 2017 - Plotroc A tool for plotting ROC curves.pdf:pdf},
issn = {15487660},
journal = {Journal of Statistical Software},
keywords = {Graphics,Interactive,Plots,ROC curves},
month = {aug},
number = {1},
pages = {1--19},
publisher = {American Statistical Association},
title = {{Plotroc: A tool for plotting ROC curves}},
url = {https://www.jstatsoft.org/index.php/jss/article/view/v079c02/v79c02.pdf https://www.jstatsoft.org/index.php/jss/article/view/v079c02},
volume = {79},
year = {2017}
}
@article{Breiman2001,
abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund & R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, * * *, 148-156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
author = {Breiman, Leo},
doi = {10.1023/A:1010933404324},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Breiman - 2001 - Random forests.pdf:pdf},
issn = {08856125},
journal = {Machine Learning},
keywords = {Classification,Ensemble,Regression},
month = {oct},
number = {1},
pages = {5--32},
publisher = {Springer},
title = {{Random forests}},
url = {https://link.springer.com/article/10.1023/A:1010933404324},
volume = {45},
year = {2001}
}
@book{Therneau2000,
address = {New York},
author = {Therneau, Terry M. and Grambsch, Patricia M.},
edition = {1},
isbn = {0-387-98784-3},
publisher = {Springer Verlag},
title = {{Modeling Survival Data: Extending the Cox Model}},
year = {2000}
}
@article{Royston2013,
abstract = {Background: A prognostic model should not enter clinical practice unless it has been demonstrated that it performs a useful role. External validation denotes evaluation of model performance in a sample independent of that used to develop the model. Unlike for logistic regression models, external validation of Cox models is sparsely treated in the literature. Successful validation of a model means achieving satisfactory discrimination and calibration (prediction accuracy) in the validation sample. Validating Cox models is not straightforward because event probabilities are estimated relative to an unspecified baseline function. Methods. We describe statistical approaches to external validation of a published Cox model according to the level of published information, specifically (1) the prognostic index only, (2) the prognostic index together with Kaplan-Meier curves for risk groups, and (3) the first two plus the baseline survival curve (the estimated survival function at the mean prognostic index across the sample). The most challenging task, requiring level 3 information, is assessing calibration, for which we suggest a method of approximating the baseline survival function. Results: We apply the methods to two comparable datasets in primary breast cancer, treating one as derivation and the other as validation sample. Results are presented for discrimination and calibration. We demonstrate plots of survival probabilities that can assist model evaluation. Conclusions: Our validation methods are applicable to a wide range of prognostic studies and provide researchers with a toolkit for external validation of a published Cox model. {\textcopyright} 2013 Royston and Altman; licensee BioMed Central Ltd.},
author = {Royston, Patrick and Altman, Douglas G.},
doi = {10.1186/1471-2288-13-33},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Royston, Altman - 2013 - External validation of a Cox prognostic model Principles and methods.pdf:pdf},
issn = {14712288},
journal = {BMC Medical Research Methodology},
keywords = {Calibration,Cox proportional hazards model,Discrimination,External validation,Prognostic models,Time to event data},
month = {dec},
number = {1},
pages = {33},
pmid = {23496923},
publisher = {BioMed Central},
title = {{External validation of a Cox prognostic model: Principles and methods}},
url = {https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-13-33},
volume = {13},
year = {2013}
}
@article{Simon2011,
abstract = {Developments in whole genome biotechnology have stimulated statistical focus on prediction methods. We review here methodology for classifying patients into survival risk groups and for using cross-validation to evaluate such classifications. Measures of discrimination for survival risk models include separation of survival curves, time-dependent ROC curves and Harrell's concordance index. For high-dimensional data applications, however, computing these measures as re-substitution statistics on the same data used for model development results in highly biased estimates. Most developments in methodology for survival risk modeling with high-dimensional data have utilized separate test data sets for model evaluation. Cross-validation has sometimes been used for optimization of tuning parameters. In many applications, however, the data available are too limited for effective division into training and test sets and consequently authors have often either reported re-substitution statistics or analyzed their data using binary classification methods in order to utilize familiar cross-validation. In this article we have tried to indicate how to utilize cross-validation for the evaluation of survival risk models; specifically how to compute cross-validated estimates of survival distributions for predicted risk groups and how to compute cross-validated time-dependent ROC curves. We have also discussed evaluation of the statistical significance of a survival risk model and evaluation of whether high-dimensional genomic data adds predictive accuracy to a model based on standard covariates alone. Published by Oxford University Press 2011.},
author = {Simon, Richard M. and Subramanian, Jyothi and Li, Ming Chung and Menezes, Supriya},
doi = {10.1093/bib/bbr001},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Simon et al. - 2011 - Using cross-validation to evaluate predictive accuracy of survival risk classifiers based on high-dimensional data.pdf:pdf},
issn = {14675463},
journal = {Briefings in Bioinformatics},
keywords = {Cross-validation,Gene expression,Predictive medicine,Survival risk classification},
month = {may},
number = {3},
pages = {203--214},
pmid = {21324971},
publisher = {Oxford University Press},
title = {{Using cross-validation to evaluate predictive accuracy of survival risk classifiers based on high-dimensional data}},
url = {/pmc/articles/PMC3105299/ /pmc/articles/PMC3105299/?report=abstract https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3105299/},
volume = {12},
year = {2011}
}
@article{Harrington1982,
author = {Harrington, David P. and Fleming, Thomas R.},
doi = {10.2307/2335991},
issn = {00063444},
journal = {Biometrika},
month = {dec},
number = {3},
pages = {553},
publisher = {JSTOR},
title = {{A Class of Rank Test Procedures for Censored Survival Data}},
volume = {69},
year = {1982}
}
@article{Harrell1996,
abstract = {Multivariable regression models are powerful tools that are used frequently in studies of clinical outcomes. These models can use a mixture of categorical and continuous variables and can handle partially observed (censored) responses. However, uncritical application of modelling techniques can result in models that poorly fit the dataset at hand, or, even more likely, inaccurately predict outcomes on new subjects. One must know how to measure qualities of a model's fit in order to avoid poorly fitted or overfitted models. Measurement of predictive accuracy can be difficult for survival time data in the presence of censoring. We discuss an easily interpretable index of predictive discrimination as well as methods for assessing calibration of predicted survival probabilities. Both types of predictive accuracy should be unbiasedly validated using bootstrapping or cross-validation, before using predictions in a new data series. We discuss some of the hazards of poorly fitted and overfitted regression models and present one modelling strategy that avoids many of the problems discussed. The methods described are applicable to all regression models, but are particularly needed for binary, ordinal, and time-to-event outcomes. Methods are illustrated with a survival analysis in prostate cancer using Cox regression.},
author = {Harrell, Frank E. and Lee, Kerry L. and Mark, Daniel B.},
doi = {10.1002/(SICI)1097-0258(19960229)15:4<361::AID-SIM168>3.0.CO;2-4},
issn = {02776715},
journal = {Statistics in Medicine},
keywords = {pmid:8668867, doi:10.1002/(SICI)1097-0258(19960229},
month = {feb},
number = {4},
pages = {361--387},
pmid = {8668867},
publisher = {Stat Med},
title = {{Multivariable prognostic models: Issues in developing models, evaluating assumptions and adequacy, and measuring and reducing errors}},
url = {https://pubmed.ncbi.nlm.nih.gov/8668867/},
volume = {15},
year = {1996}
}
@article{Hahsler2019,
abstract = {This article describes the implementation and use of the R package dbscan, which provides complete and fast implementations of the popular density-based clustering algorithm DBSCAN and the augmented ordering algorithm OPTICS. Package dbscan uses advanced open-source spatial indexing data structures implemented in C++ to speed up computation. An important advantage of this implementation is that it is up-to-date with several improvements that have been added since the original algorithms were publications (e.g., artifact corrections and dendrogram extraction methods for OPTICS). We provide a consistent presentation of the DBSCAN and OPTICS algorithms, and compare dbscan's implementation with other popular libraries such as the R package fpc, ELKI, WEKA, PyClustering, SciKit-Learn, and SPMF in terms of available features and using an experimental comparison.},
author = {Hahsler, Michael and Piekenbrock, Matthew and Doran, Derek},
doi = {10.18637/jss.v091.i01},
issn = {15487660},
journal = {Journal of Statistical Software},
keywords = {DBSCAN,Density-based clustering,Hierarchical clustering,OPTICS},
month = {oct},
pages = {1--30},
publisher = {American Statistical Association},
title = {{Dbscan: Fast density-based clustering with R}},
url = {https://www.jstatsoft.org/index.php/jss/article/view/v091i01},
volume = {91},
year = {2019}
}
